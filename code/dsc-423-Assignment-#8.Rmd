---
output:
  pdf_document: default
  html_document: default
---
# DSC 423: Data Analysis & Regression
## Assignment 8: Residual Analysis
~~~
Name: Kushal Navghare
Student ID: 2116916
Honor Statement: I, Kushal Navghare, assure that I have completed this work independently.
The solutions given are entirely my own work.
~~~

```{r echo=FALSE, include=FALSE}
rm(list=ls())
dir.path <- '/home/kush/DSC-423_Data_Analysis_Regression_I/'
setwd(dir.path)
source(paste0(dir.path, "code/00_Setup.R"))

library(MASS)
library(Metrics)
library(car)
library(corrplot)
library(DAAG)
```

### 1. Short Essay - Read the short PDF on George Box. Explain in your own words the significance of “all models are wrong, but some are useful” as if you were interviewing for job in data science.

Ans: Author Guillem Barroso talks about how the results generated by all the models are all wrong, however, some models can be helpful in certain situations. He states that all models are flawed and can never fully mimic the reality. However, despite their imperfections, models can still be valuable if they are close enough to the real world. While these models will never be perfect, they can provide useful insights and will be helpful decision-making. It also helps if we understand the requirement of the real world application of the model results. Not all situations require a high accuracy model. The focus should be on developing models that provides an estimated outcome according to the specific situation. The role of human interpretation in the results produced by models is also important. A data scientist should know how to not only build models, but also how to convey the results produced by these models to the end users. A data scientists should be able to analyze and make sense of the model outputs in relation to the audience, context, objectives, and constraints of the problem at hand. 

### 2. Previously, you used the PGA tour dataset to predict Prize Money. Use a log transformation to transform Prize Money into a new response variable. Apply your knowledge of regression analysis to fit a regression model using the remaining predictors in your dataset. If necessary, remove the non-significant variables. Remember to remove one variable at a time (variable with largest p- value is removed first) and refit the model, until all variables are significant. 

```{r}
# read file
raw_df <- read.csv(paste0(dir.path, 'data/pgatour2006.csv'))

# summary
dim(raw_df)
summary(raw_df)
str(raw_df)
```

#### a. (10 points) Check for multicollinear. Explain your process.

For understanding the multicolinearity, we will first build a full model using all features to predict PrizeMoney. Then we will remove the features from the model based on the significance values (p-values) of the features starting from highest.

To understand multicolinearity, we will use VIF values (Variance Inflation Factor) to assess the multiconilearity. These values quantifies how much the variance of the estimated regression coefficients is increased due to multicollinearity. A high VIF value indicates a high degree of multicollinearity, suggesting that the corresponding predictor variables are strongly correlated with each other.

```{r}
# select numeric columns only
num_df <- raw_df %>% 
  select_if(is.numeric) %>% 
  dplyr::select(-c(PuttingAverage, BounceBack, DrivingAccuracy,
                   AveDrivingDistance, PuttsPerRound))

# full model
full_model <- lm(log(PrizeMoney)~., data = raw_df %>% dplyr::select(-c(Name)))

# build final model
final_model <- lm(log(PrizeMoney)~., data = num_df)

# model output
summary(final_model)
```

```{r}
# multicolinearity in full model
vif(full_model)
```
As seen in full model, we can see the VIF (Variance Inflation Factor) for few features is quite high(PuttsPerRound, PuttingAverage, GIR, etc.) These variables are likely contributing to multicollinearity issues. A general rule is to prioritize addressing variables with VIF values above the chosen threshold (e.g., 5 or 10).

### b. Compare this model to the one you made in the previous assignment. How did performing a log transformation impact the quality of the model? Why?

Previously, we have used a model with below formula:

PrizeMoney~DrivingAccuracy^2+GIR^2+BirdieConversion^2+Scrambling^2

```{r}
# model (previous)
prev_model <- lm(PrizeMoney~DrivingAccuracy^2+GIR^2+BirdieConversion^2+Scrambling^2,
                 data = raw_df %>% dplyr::select(-c(Name)))

summary(prev_model)
```

With this model, we were able to achieve 0.38 of adj R-squared error. Now, after applying log(PrizeMoney) on the outcome, we achieved 0.55 adj R-squared. This shows improvement in performance after applying log transformation to the outcome. This can happen when one or more assumptions about the linear regression are violated. It may be because the relationship between the predictors and the outcome may not be strictly linear. Or the assumption of constant variance of the errors across all levels of the predictors. In some cases, the variance of the response variable (y) may increase or decrease systematically with its mean value. 

### c. Analyze and discuss the residual plots.

```{r fig.align='center', fig.width=6, fig.height=4}
# residual plots
residuals <- as.data.frame(final_model$residuals)

plt_1 <- ggplot(data = residuals, aes(x = final_model$residuals))
plt_1 + geom_histogram( binwidth = .45) + 
  xlab("residuals") + 
  ylab("count") + 
  ggtitle("Distribution of residuals") 
```

Now, let's look at residual plots with z-score.

```{r}
# calculate z-score
avg_resid <- mean(final_model$residuals)
std_resid <- sd(final_model$residuals)

z_score_resid <- (final_model$residuals - avg_resid)/std_resid

num_df <- num_df %>% 
  mutate(z_score_resid = z_score_resid)
```

Now, we will look at the plots.

```{r fig.align='center', fig.height=4, fig.width=6}
plt_2 <- ggplot(data = num_df, aes(x = GIR, y = z_score_resid))
plt_2 + geom_point() + 
  ggtitle("plot: GIR vs z_score")

plt_3 <- ggplot(data = num_df, aes(x = BirdieConversion, y = z_score_resid))
plt_3 + geom_point() + 
  ggtitle("plot: BirdieConversion vs z_score")

plt_4 <- ggplot(data = num_df, aes(x = SandSaves, y = z_score_resid))
plt_4 + geom_point() + 
  ggtitle("plot: SandSaves vs z_score")

plt_4 <- ggplot(data = num_df, aes(x = Scrambling, y = z_score_resid))
plt_4 + geom_point() + 
  ggtitle("plot: Scrambling vs z_score")
```

Looking at above residual plots, the residuals does not seem to have a constant variance. Which probably explains the low adj R-squared. Also, the line of fit is influenced by few outliers in the dataset. 

### d. Analyze if there are any outliers and/or influential points. If there are points in the dataset that need to be investigated, give one or more reason to support each point chosen. Discuss your answer.

Ans: Outliers in the dataset can affect the line of fit, it will change the slope of the line by trying to fit the outlier. The outliers can be found in the model plots.

```{r fig.align='center', fig.height=4, fig.width=6}
plot(final_model)
```

From above plots, it is clear that observation #180, #185 & #47 are outliers in the dataset. 1. If looked at the final plot, the points are annotated. 
2. Normal Q-Q plot also shows that the residual are normal, except of these 3 outliers which influence the best line of fit.

Let's see these outliers and their distribution in the data are affecting the model performance. We will test by removing such cases from the data.

```{r}
# remove outlier rows
df_no_outliers <- num_df[-c(47, 180, 185), ] %>% 
  dplyr::select(-c(z_score_resid))

# model without outliers
model_no_outlier <- lm(log(PrizeMoney)~., data = df_no_outliers)

summary(model_no_outlier)
```

From the results, it looks like the performance improved slightly. However, there are new outliers which are shown in model plots.

```{r fig.align='center', fig.width=6, fig.height=4}
plot(model_no_outlier)
```
